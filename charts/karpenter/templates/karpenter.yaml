---
# Source: karpenter/templates/poddisruptionbudget.yaml
#apiVersion: policy/v1
#kind: PodDisruptionBudget
#metadata:
#  name: karpenter
#  namespace: default
#  labels:
#    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
#    app.kubernetes.io/name: karpenter
#    app.kubernetes.io/instance: karpenter
#    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
#    app.kubernetes.io/managed-by: Helm
#spec:
#  maxUnavailable: 1
#  selector:
#    matchLabels:
#      app.kubernetes.io/name: karpenter
#      app.kubernetes.io/instance: karpenter
#---
# Source: karpenter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: karpenter
  namespace: default
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
  annotations:
    eks.amazonaws.com/role-arn: arn:{{ .Values.awsPartition }}:iam::{{ .Values.awsAccountId }}:role/KarpenterControllerRole-{{ .Values.karpenterVersion }}
---
# Source: karpenter/templates/aggregate-clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: karpenter-admin
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: ["karpenter.sh"]
    resources: ["nodepools", "nodepools/status", "nodeclaims", "nodeclaims/status"]
    verbs: ["get", "list", "watch", "create", "delete", "patch"]
  - apiGroups: ["karpenter.k8s.aws"]
    resources: ["ec2nodeclasses"]
    verbs: ["get", "list", "watch", "create", "delete", "patch"]
---
# Source: karpenter/templates/clusterrole-core.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: karpenter-core
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
rules:
  # Read
  - apiGroups: ["karpenter.sh"]
    resources: ["nodepools", "nodepools/status", "nodeclaims", "nodeclaims/status"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["pods", "nodes", "persistentvolumes", "persistentvolumeclaims", "replicationcontrollers", "namespaces"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "volumeattachments"]
    verbs: ["get", "watch", "list"]
  - apiGroups: ["apps"]
    resources: ["daemonsets", "deployments", "replicasets", "statefulsets"]
    verbs: ["list", "watch"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["get", list, "watch"]
  # Write
  - apiGroups: ["karpenter.sh"]
    resources: ["nodeclaims", "nodeclaims/status"]
    verbs: ["create", "delete", "update", "patch"]
  - apiGroups: ["karpenter.sh"]
    resources: ["nodepools", "nodepools/status"]
    verbs: ["update", "patch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["patch", "delete", "update"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["delete"]
---
# Source: karpenter/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: karpenter
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
rules:
  # Read
  - apiGroups: ["karpenter.k8s.aws"]
    resources: ["ec2nodeclasses"]
    verbs: ["get", "list", "watch"]
  # Write
  - apiGroups: ["karpenter.k8s.aws"]
    resources: ["ec2nodeclasses", "ec2nodeclasses/status"]
    verbs: ["patch", "update"]
---
# Source: karpenter/templates/clusterrole-core.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: karpenter-core
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: karpenter-core
subjects:
  - kind: ServiceAccount
    name: karpenter
    namespace: default
---
# Source: karpenter/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: karpenter
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: karpenter
subjects:
  - kind: ServiceAccount
    name: karpenter
    namespace: default
---
# Source: karpenter/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: karpenter
  namespace: default
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
rules:
  # Read
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "watch"]
  # Write
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["patch", "update"]
    resourceNames:
      - "karpenter-leader-election"
  # Cannot specify resourceNames on create
  # https://kubernetes.io/docs/reference/access-authn-authz/rbac/#referring-to-resources
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
---
# Source: karpenter/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: karpenter-dns
  namespace: kube-system
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
rules:
  # Read
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["kube-dns"]
    verbs: ["get"]
---
# Source: karpenter/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: karpenter
  namespace: default
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: karpenter
subjects:
  - kind: ServiceAccount
    name: karpenter
    namespace: default
---
# Source: karpenter/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: karpenter-dns
  namespace: kube-system
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: karpenter-dns
subjects:
  - kind: ServiceAccount
    name: karpenter
    namespace: default
---
# Source: karpenter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: karpenter
  namespace: default
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 8080
      targetPort: http-metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
---
# Source: karpenter/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: karpenter
  namespace: default
  labels:
    helm.sh/chart: karpenter-{{ .Values.karpenterVersion }}
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/version: "{{ .Values.karpenterVersion }}"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  revisionHistoryLimit: 10
  strategy:
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: karpenter
      app.kubernetes.io/instance: karpenter
  template:
    metadata:
      labels:
        app.kubernetes.io/name: karpenter
        app.kubernetes.io/instance: karpenter
      annotations:
    spec:
      serviceAccountName: karpenter
      securityContext:
        fsGroup: 65532
      priorityClassName: "system-cluster-critical"
      dnsPolicy: ClusterFirst
      schedulerName: "default-scheduler"
      containers:
        - name: controller
          securityContext:
            runAsUser: 65532
            runAsGroup: 65532
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
          image: public.ecr.aws/karpenter/controller:{{ .Values.karpenterVersion }}
          imagePullPolicy: IfNotPresent
          env:
            - name: KUBERNETES_MIN_VERSION
              value: "1.19.0-0"
            - name: KARPENTER_SERVICE
              value: karpenter
            - name: LOG_LEVEL
              value: "info"
            - name: LOG_OUTPUT_PATHS
              value: "stdout"
            - name: LOG_ERROR_OUTPUT_PATHS
              value: "stderr"
            - name: METRICS_PORT
              value: "8080"
            - name: HEALTH_PROBE_PORT
              value: "8081"
            - name: SYSTEM_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MEMORY_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: controller
                  divisor: "0"
                  resource: limits.memory
            - name: FEATURE_GATES
              value: "SpotToSpotConsolidation=false,NodeRepair=false"
            - name: BATCH_MAX_DURATION
              value: "10s"
            - name: BATCH_IDLE_DURATION
              value: "1s"
            - name: VM_MEMORY_OVERHEAD_PERCENT
              value: "0.075"
            - name: RESERVED_ENIS
              value: "0"
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8081
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 30
            httpGet:
              path: /healthz
              port: http
          readinessProbe:
            initialDelaySeconds: 5
            timeoutSeconds: 30
            httpGet:
              path: /readyz
              port: http
          resources:
            limits:
              cpu: 1
              memory: 1Gi
            requests:
              cpu: 1
              memory: 1Gi
      nodeSelector:
        kubernetes.io/os: linux
      # The template below patches the .Values.affinity to add a default label selector where not specificed
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: karpenter.sh/nodepool
                operator: DoesNotExist
              - key: eks.amazonaws.com/nodegroup
                operator: In
                values:
                - {{ .Values.nodegroupName }}
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/instance: karpenter
                app.kubernetes.io/name: karpenter
            topologyKey: kubernetes.io/hostname
      # The template below patches the .Values.topologySpreadConstraints to add a default label selector where not specificed
      topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/instance: karpenter
              app.kubernetes.io/name: karpenter
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
