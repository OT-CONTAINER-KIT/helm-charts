# Custom values for your chart
clusterName: ""  # Name of the EKS cluster (for identification in the chart and Karpenter)
awsPartition: ""  # AWS partition, default is 'aws' (used in multi-region or partitioned environments)
awsAccountId: 3333  # AWS account ID where the resources will be provisioned

# Karpenter chart overrides
karpenter:
  settings:
    clusterName: ""  # Cluster name for the Karpenter controller to identify and manage nodes in this cluster

  serviceAccount:
    annotations:
      eks.amazonaws.com/role-arn: #arn:aws:iam::3333:role/KarpenterControllerRole-demo-eks  # IAM role ARN for Karpenter controller's access to AWS services

  # Karpenter controller resources can be customized in this section below 
  # controller:
  #   resources:
  #     requests:
  #       cpu: "1"  # CPU resource request for the Karpenter controller (minimum resources Karpenter will be allocated)
  #       memory: "1Gi"  # Memory resource request for the Karpenter controller
  #     limits:
  #       cpu: "1"  # CPU resource limit for the Karpenter controller (maximum resources Karpenter can consume)
  #       memory: "1Gi"  # Memory resource limit for the Karpenter controller


ec2NodeClasses:
  - name: default
    amiFamily: AL2s
    role: "KarpenterNodeRole-my-eks-cluster" # Name of karpenter Node Role ( NOT THE ARN )
    amiSelector:
      arm: ami-arm-id # ARM_AMI_ID="$(aws ssm get-parameter --name /aws/service/eks/optimized-ami/${K8S_VERSION}/amazon-linux-2-arm64/recommended/image_id --query Parameter.Value --output text)"
      amd: ami-amd-id # AMD_AMI_ID="$(aws ssm get-parameter --name /aws/service/eks/optimized-ami/${K8S_VERSION}/amazon-linux-2/recommended/image_id --query Parameter.Value --output text)"
      gpu: ami-gpu-id # GPU_AMI_ID="$(aws ssm get-parameter --name /aws/service/eks/optimized-ami/${K8S_VERSION}/amazon-linux-2-gpu/recommended/image_id --query Parameter.Value --output text)"



# NodePools define groups of nodes with specific requirements
nodePools:
  - name: default  # Name of the node pool, preset here is set to default nodepool
    requirements:  # List of node requirements for scheduling
      - key: kubernetes.io/arch  # Architecture requirement (e.g., amd64, arm64)
        operator: In  # Only nodes with the specified architecture will be selected
        values:
          - "amd64"  # Specifies that the node should have an amd64 architecture
      - key: kubernetes.io/os  # OS requirement (e.g., linux, windows)
        operator: In  # Only nodes with the specified OS will be selected
        values:
          - "linux"  # Specifies that the node should run Linux
      - key: karpenter.sh/capacity-type  # Defines the instance's capacity type
        operator: In  # Only nodes with the specified capacity type will be selected
        values:
          - "on-demand"  # Specifies that the node should be an on-demand instance, can be "spot" as well 
      - key: karpenter.k8s.aws/instance-category  # Defines the instance category (e.g., t, m, r)
        operator: In  # Only nodes with the specified instance category will be selected
        values:
          - "t"  # These can be customized as per need 
          - "m"  
          - "r"  
      - key: karpenter.k8s.aws/instance-generation  # Instance generation requirement
        operator: Gt  # Greater than the specified value
        values:
          - "2"  # Specifies that only instance generations greater than 2 are allowed
    nodeClass:  # Defines the node class, which is linked to EC2NodeClass
      group: karpenter.k8s.aws  # Group of the EC2NodeClass
      kind: EC2NodeClass  # Type of node class, which is EC2NodeClass in this case
      name: default  # Name of the EC2NodeClass to use for the node pool (name of the EC2 instance class)
    expireAfter: 720h  # Maximum lifetime of the node pool before it expires (720 hours = 30 days)
    limits:  # Resource limits for the node pool
      cpu: "1000"  # Maximum CPU limit for the node pool
      #memory: "1Gi"
    disruption:  # Policy for handling disruption in the node pool
      consolidationPolicy: WhenEmptyOrUnderutilized  # Consolidate nodes when they are empty or underutilized
      consolidateAfter: 1m  # Time after which consolidation will occur, in this case, 1 minute
    #Uncomment Below annotations key ( next 3 Lines ) if you want to use annotations 
    # annotations:  # Annotations are key-value pairs that provide additional metadata for the node pool
    #   example.com/owner: "my-team"  # An example annotation that associates the node pool with a team
    #   example.com/maintainer: "admin@company.com"  # Example annotation for the maintainer's contact information
    #Uncomment below taint key ( next 4 Lines ) if you want to use taints 
    # taints:  # Taints are used to control which pods can be scheduled on the node pool
    #   - key: "example.com/special-taint"  # Taint key that identifies the taint
    #     value: "special-value"  # Value associated with the taint
    #     effect: "NoExecute"  # Effect of the taint. In this case, NoExecute means pods won't be scheduled on tainted nodes
    # Comment Labels Key below if you dont want to use Labels 
    labels:  # Labels are key-value pairs used for categorizing the node pool
      environment: production  # Label indicating that this node pool is for production use
      team: "engineering"  # Label associating the node pool with the engineering team
