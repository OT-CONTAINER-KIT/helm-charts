apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kube-node-high-rules
spec:
  groups:
  - name: ds-jupiter-kube-node-high-rules
    rules:
    - alert: HostOutOfMemory | PROD
      expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
      for: 5m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host out of memory (instance {{ $labels.instance }})
        description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    - alert: HostMemoryUnderMemoryPressure | PROD
      expr: rate(node_vmstat_pgmajfault[1m]) > 500
      for: 2m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host memory under memory pressure (instance {{ $labels.instance }})
        description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}"
    - alert: HostUnusualNetworkThroughputIn | PROD
      expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 300
      for: 5m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host unusual network throughput in (instance {{ $labels.instance }})
        description: "Host network interfaces are probably receiving too much data (> 50 MB/s)\n  VALUE = {{ $value }}"
    - alert: HostUnusualNetworkThroughputOut | PROD
      expr: sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 300
      for: 5m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host unusual network throughput out (instance {{ $labels.instance }})
        description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}"
    - alert: HostUnusualDiskReadRate | PROD
      expr: sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 300
      for: 10m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host unusual disk read rate (instance {{ $labels.instance }})
        description: "Disk is probably reading too much data (> 100 MB/s)\n  VALUE = {{ $value }}"
    - alert: HostUnusualDiskWriteRate | PROD
      expr: sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 200
      for: 10m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host unusual disk write rate (instance {{ $labels.instance }})
        description: "Disk is probably writing too much data (> 100 MB/s)\n  VALUE = {{ $value }}"
    - alert: HostUnusualDiskReadLatency | PROD
      expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.5 and rate(node_disk_reads_completed_total[1m]) > 0
      for: 2m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host unusual disk read latency (instance {{ $labels.instance }})
        description: "Disk latency is growing (read operations > 500ms)\n  VALUE = {{ $value }}"
    - alert: HostUnusualDiskWriteLatency | PROD
      expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.5 and rate(node_disk_writes_completed_total[1m]) > 0
      for: 2m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host unusual disk write latency (instance {{ $labels.instance }})
        description: "Disk latency is growing (write operations > 500ms)\n  VALUE = {{ $value }}"
    - alert:  HostHighCpuLoad | PROD
      expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
      for: 5m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host high CPU load (instance {{ $labels.instance }})
        description: "CPU load is > 95%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    - alert: HostSwapIsFillingUp | PROD
      expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 70
      for: 2m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host swap is filling up (instance {{ $labels.instance }})
        description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    - alert: KubePersistentVolumeFillingUp | PROD
      annotations:
        description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefillingup
        summary: PersistentVolume is filling up.
      expr: |-
        kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
          /
        kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
          < 0.03
      for: 1m
      labels:
        severity: warning
        channel: slack
        team: devops
    - alert: KubePersistentVolumeErrors | PROD
      annotations:
        description: The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
        summary: PersistentVolume is having issues with provisioning.
      expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
      for: 1m
      labels:
        severity: warning
        channel: slack
        team: devops
    - alert: NodeNetworkInterfaceFlapping | PROD
      annotations:
        message: Network interface "{{ $labels.device }}" changing it's up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
      expr: changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
      for: 2m
      labels:
        severity: warning
        channel: slack
        team: devops
    # - alert: NodeFilesystemSpaceFillingUp | PROD
    #   annotations:
    #     description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up.
    #     runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
    #     summary: Filesystem is predicted to run out of space within the next 24 hours.
    #   expr: |-
    #     (
    #       node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 40
    #     and
    #       predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
    #     and
    #       node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
    #     )
    #   for: 1h
    #   labels:
    #     severity: warning
    #     channel: slack
    #     team: devops
    - alert: NodeFilesystemSpaceFillingUp | PROD
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up fast.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of space within the next 4 hours.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 15
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        channel: slack
        team: devops
#     - alert: NodeFilesystemFilesFillingUp | PROD
#       annotations:
#         description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up.
#         runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
#         summary: Filesystem is predicted to run out of inodes within the next 24 hours.
#       expr: |-
#         (
#           node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 40
#         and
#           predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
#         and
#           node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
#         )
#       for: 1h
#       labels:
#         severity: warning
#         channel: slack
#         team: devops
    - alert: NodeFilesystemFilesFillingUp | PROD
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
        summary: Filesystem is predicted to run out of inodes within the next 4 hours.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 20
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        channel: slack
        team: devops
    - alert: NodeNetworkReceiveErrs | PROD
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last two minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworkreceiveerrs
        summary: Network interface is reporting many receive errors.
      expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
        channel: slack
        team: devops
    - alert: NodeNetworkTransmitErrs | PROD
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworktransmiterrs
        summary: Network interface is reporting many transmit errors.
      expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
        channel: slack
        team: devops
    - alert: HostOomKillDetected | PROD
      expr: increase(node_vmstat_oom_kill[1m]) > 5
      for: 0m
      labels:
        severity: warning
        channel: slack
        team: devops
      annotations:
        summary: Host OOM kill detected (instance {{ $labels.instance }}) 2 times in last 1 minute
        description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
